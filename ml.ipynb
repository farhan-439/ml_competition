{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233cd525",
   "metadata": {},
   "source": [
    "#Rock-Paper-Scissors\n",
    "\n",
    "## IMPORTING MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458d4d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Running on\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d7297",
   "metadata": {},
   "source": [
    "## Load and Inspect Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45283690",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m imgs1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m train_data])   \u001b[38;5;66;03m# (N,24,24)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m imgs2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m train_data])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core.numeric'"
     ]
    }
   ],
   "source": [
    "with open('train.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "imgs1 = np.stack([t[0] for t in train_data])   # (N,24,24)\n",
    "imgs2 = np.stack([t[1] for t in train_data])\n",
    "labels = np.array([t[2] for t in train_data])  # +1 / -1\n",
    "train_ids = np.array([t[3] for t in train_data])\n",
    "print(\"Train:\", imgs1.shape, \"Label counts:\", np.unique(labels, return_counts=True))\n",
    "\n",
    "with open('test.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "test_imgs1 = np.stack([t[0] for t in test_data])\n",
    "test_imgs2 = np.stack([t[1] for t in test_data])\n",
    "test_ids   = np.array([t[3] for t in test_data])\n",
    "print(\"Test:\", test_imgs1.shape, \"# IDs:\", len(test_ids))\n",
    "\n",
    "# Combine into shape (N,2,24,24)\n",
    "X = np.stack([imgs1, imgs2], axis=1)      # train pairs\n",
    "X_test = np.stack([test_imgs1, test_imgs2], axis=1)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 3. Train/Validation Split\n",
    "# ========================================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "print(\"Train split:\", X_train.shape, \"Val split:\", X_val.shape)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 4. Simple CNN Definition\n",
    "# ========================================\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input: (batch,2,24,24)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),   # →16×12×12\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # →32×6×6\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*6*6, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.fc(x).squeeze(-1)  # logits\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 5. PyTorch Dataset & DataLoader\n",
    "# ========================================\n",
    "class RPSDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.from_numpy(self.X[idx])            # (2,24,24)\n",
    "        if self.y is None:\n",
    "            return img\n",
    "        lbl = 1.0 if self.y[idx] > 0 else 0.0          # +1→1, -1→0\n",
    "        return img, torch.tensor(lbl, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 6. CNN Training Function\n",
    "# ========================================\n",
    "def train_cnn(model, train_loader, val_loader, epochs=10, lr=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_acc, best_wts = 0.0, None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_lbls = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb).cpu().numpy()\n",
    "                probs = 1 / (1 + np.exp(-logits))\n",
    "                all_preds.extend(probs)\n",
    "                all_lbls.extend(yb.numpy())\n",
    "        bin_preds = [1 if p>0.5 else 0 for p in all_preds]\n",
    "        acc = accuracy_score(all_lbls, bin_preds)\n",
    "        print(f\"Epoch {ep}/{epochs} — val_acc: {acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_wts = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "    model.load_state_dict(best_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 7. Train Two CNNs (Different Seeds)\n",
    "# ========================================\n",
    "bs = 64\n",
    "train_ds = RPSDataset(X_train, y_train)\n",
    "val_ds   = RPSDataset(X_val,   y_val)\n",
    "train_ld = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "val_ld   = DataLoader(val_ds,   batch_size=bs)\n",
    "\n",
    "cnn_models = []\n",
    "for seed in [0, 1]:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    print(f\"\\n> Training CNN with seed={seed}\")\n",
    "    net = SimpleCNN()\n",
    "    net = train_cnn(net, train_ld, val_ld, epochs=8, lr=1e-3)\n",
    "    cnn_models.append(net)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 8. Extract CNN Probabilities\n",
    "# ========================================\n",
    "def get_probs(model, X_arr):\n",
    "    ds = RPSDataset(X_arr, y=None)\n",
    "    ld = DataLoader(ds, batch_size=bs)\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb in ld:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb).cpu().numpy()\n",
    "            probs.extend(1 / (1 + np.exp(-logits)))\n",
    "    return np.array(probs)\n",
    "\n",
    "train_cnn_feats = np.vstack([get_probs(m, X_train) for m in cnn_models]).T\n",
    "val_cnn_feats   = np.vstack([get_probs(m, X_val)   for m in cnn_models]).T\n",
    "test_cnn_feats  = np.vstack([get_probs(m, X_test)  for m in cnn_models]).T\n",
    "print(\"CNN feature shapes:\", train_cnn_feats.shape, val_cnn_feats.shape)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 9. Boosted Trees on Raw-Pixel Differences\n",
    "# ========================================\n",
    "def make_flat(X_arr):\n",
    "    dif = X_arr[:,0] - X_arr[:,1]      # (N,24,24)\n",
    "    return dif.reshape(len(dif), -1)   # (N,576)\n",
    "\n",
    "Xtr_flat = make_flat(X_train)\n",
    "Xvl_flat = make_flat(X_val)\n",
    "Xte_flat = make_flat(X_test)\n",
    "\n",
    "gbm = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "gbm.fit(Xtr_flat, (y_train>0).astype(int))\n",
    "boost_tr_p = gbm.predict_proba(Xtr_flat)[:,1]\n",
    "boost_vl_p = gbm.predict_proba(Xvl_flat)[:,1]\n",
    "boost_te_p = gbm.predict_proba(Xte_flat)[:,1]\n",
    "\n",
    "print(\"GBM val acc:\", \n",
    "      accuracy_score((y_val>0).astype(int), boost_vl_p>0.5))\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 10. Stacked Meta-Learner\n",
    "# ========================================\n",
    "stack_tr = np.column_stack([train_cnn_feats, boost_tr_p])\n",
    "stack_vl = np.column_stack([val_cnn_feats,   boost_vl_p])\n",
    "stack_te = np.column_stack([test_cnn_feats,  boost_te_p])\n",
    "\n",
    "meta = LogisticRegression()\n",
    "meta.fit(stack_tr, (y_train>0).astype(int))\n",
    "\n",
    "vl_pred = meta.predict(stack_vl)\n",
    "print(\"Meta val acc:\", accuracy_score((y_val>0).astype(int), vl_pred))\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 11. Final Test Predictions & Submission\n",
    "# ========================================\n",
    "te_pred = meta.predict(stack_te)\n",
    "te_label = np.where(te_pred>0, 1, -1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'Predicted': te_label\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Saved ▶ submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
